{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_path_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#File Path\n",
    "filename_queue_description = tf.train.string_input_producer(['./data/description/v2/coded_data.csv'])\n",
    "filepath_ckpt  = \"./ckpt/model_weight_v2_2.ckpt\" #weight saver check point file path\n",
    "filepath_pred = \"./output/predicted_v2.csv\" #predicted value file path\n",
    "num_record = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Params - IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilinear_size = 28\n",
    "resized_size = bilinear_size*bilinear_size*3\n",
    "img_label_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Params - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_vec_size = 32\n",
    "input_vec_size = 32\n",
    "batch_size = num_record\n",
    "state_size_1 = 20\n",
    "state_size_2 = 100\n",
    "hidden = 18\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg16_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "        self.fc_layers()\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            sess.run(self.parameters[i].assign(weights[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_img_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess_vgg:\n",
    "    imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])\n",
    "    vgg = vgg16(imgs, 'vgg16_weights.npz', sess_vgg)\n",
    "    img_files = ['./data/img/cropped/' + i for i in os.listdir('./data/img/cropped')]\n",
    "    imgs = [imread(file, mode='RGB') for file in img_files]\n",
    "    #bilinear feature\n",
    "    imgs_bi = [sess_vgg.run(vgg.fc1, feed_dict={vgg.imgs: [img]})[0] for img in imgs]\n",
    "#     imgs_bi = [imresize(arr=img, interp='bilinear', size=bilinear_size) for img in imgs]\n",
    "    imgs_bi = np.reshape(a=imgs_bi, newshape=[50,-1])\n",
    "    \n",
    "    #label\n",
    "    prob = [sess_vgg.run(vgg.probs, feed_dict={vgg.imgs: [img]})[0] for img in imgs]\n",
    "    preds = [(np.argsort(p)[::-1])[0:1] for p in prob]\n",
    "    preds = [p[0] for p in preds]\n",
    "    for i in range(len(preds)):\n",
    "        if(preds[i]==430):\n",
    "            preds[i]=0\n",
    "        elif(preds[i]==429):\n",
    "            preds[i]=1\n",
    "        elif(preds[i]==805):\n",
    "            preds[i]=2\n",
    "        elif(preds[i]==768):\n",
    "            preds[i]=3\n",
    "        elif(preds[i]==574):\n",
    "            preds[i]=4\n",
    "    \n",
    "    img_label_onehot = tf.one_hot(indices=preds, depth = 5)\n",
    "    print(preds)\n",
    "#     print(sess_vgg.run(img_label_onehot))\n",
    "    #clear\n",
    "    imgs = None\n",
    "    vgg = None\n",
    "    img_files = None\n",
    "    prob = None\n",
    "    sess_vgg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(imgs_bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text_Reader_Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader()\n",
    "key,value = reader.read(filename_queue_description)\n",
    "record_defaults =[[-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2]]\n",
    "w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18, w19 = tf.decode_csv(value, record_defaults)  \n",
    "\n",
    "feature_label = tf.stack([w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18, w19])\n",
    "feature_word = tf.stack([w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess_data:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "#     img_queue = []\n",
    "    for i in range(num_record):\n",
    "#         image = sess.run(images)\n",
    "        \n",
    "        raw_label, raw_input = sess_data.run([feature_label, feature_word])\n",
    "        onehot_input = tf.one_hot(indices=raw_input, depth=32)\n",
    "        onehot_label = tf.one_hot(indices=raw_label, depth=32)\n",
    "        if i == 0:\n",
    "            full_input = onehot_input\n",
    "            full_label = onehot_label\n",
    "        else:\n",
    "            full_input = tf.concat([full_input, onehot_input], 0)\n",
    "            full_label = tf.concat([full_label, onehot_label], 0)\n",
    "        \n",
    "    raw_label = None\n",
    "    raw_input = None\n",
    "    onehot_input = None\n",
    "    onehot_label = None\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('batch') as scope:\n",
    "    # full_label = tf.reshape(full_label, [batch_size, hidden, label_vec_size])\n",
    "    full_input = tf.reshape(full_input, [batch_size, hidden, input_vec_size])\n",
    "    full_label = tf.reshape(full_label, [batch_size, hidden, input_vec_size])\n",
    "    \n",
    "#     input_batch, label_batch = tf.train.batch([full_input, full_label], batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_1') as scope:\n",
    "    with tf.variable_scope('lstm_layer_1'):\n",
    "        rnn_cell_1 = tf.contrib.rnn.BasicLSTMCell(state_size_1, reuse=None)\n",
    "        output_1, _ = tf.contrib.rnn.static_rnn(rnn_cell_1, tf.unstack(full_input, axis=1), dtype=tf.float32)\n",
    "        input_2 = [tf.concat([out, imgs_bi, img_label_onehot], axis=1) for out in output_1]\n",
    "#         output_w_1 = tf.Variable(tf.truncated_normal([hidden, state_size_1, input_vec_size]))\n",
    "#         output_b_1 = tf.Variable(tf.zeros([input_vec_size]))\n",
    "#         pred_temp = tf.matmul(output_1, output_w_1) + output_b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_2') as scope:\n",
    "    with tf.variable_scope('lstm_layer_2'):\n",
    "        rnn_cell_2 = tf.contrib.rnn.BasicLSTMCell(state_size_2, reuse=None)\n",
    "        output_2, _ = tf.contrib.rnn.static_rnn(rnn_cell_2, tf.unstack(input_2, axis=0), dtype=tf.float32)\n",
    "        output_w_2 = tf.Variable(tf.truncated_normal([hidden, state_size_2, input_vec_size]))\n",
    "        output_b_2 = tf.Variable(tf.zeros([input_vec_size]))\n",
    "        pred = tf.nn.softmax(tf.matmul(output_2, output_w_2) + output_b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    loss = tf.constant(0, tf.float32)\n",
    "    for i in range(hidden):\n",
    "        loss += tf.losses.softmax_cross_entropy(tf.unstack(full_label, axis=1)[i], tf.unstack(pred, axis=0)[i])\n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_train = tf.Session()\n",
    "sess_train.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess_train, filepath_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10001):\n",
    "    sess_train.run(train)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"loss : \", sess_train.run(loss))\n",
    "#             print(\"pred : \", sess.run(pred))\n",
    "save_path = saver.save(sess_train, filepath_ckpt)\n",
    "print(\"= Weigths are saved in \" + filepath_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "31 fc8_b (1000,)\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess_vgg_test:\n",
    "    imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])\n",
    "    vgg = vgg16(imgs, 'vgg16_weights.npz', sess_vgg_test)\n",
    "    test_img_files = ['./data/img/test/cropped/test001.png', './data/img/cropped/005.png', './data/img/cropped/014.png', './data/img/cropped/021.png', './data/img/cropped/036.png', './data/img/cropped/050.png']\n",
    "    num_imgs = len(test_img_files)\n",
    "    test_imgs = [imread(file, mode='RGB') for file in test_img_files]\n",
    "    \n",
    "    #bilinear feature\n",
    "    test_imgs_bi = [sess_vgg_test.run(vgg.fc1, feed_dict={vgg.imgs: [img]})[0] for img in test_imgs]\n",
    "#     test_imgs_bi = [imresize(arr=img, interp='bilinear', size=bilinear_size) for img in test_imgs]\n",
    "    test_imgs_bi = np.reshape(a=test_imgs_bi, newshape=[num_imgs,-1])\n",
    " \n",
    "    #label\n",
    "    prob = [sess_vgg_test.run(vgg.probs, feed_dict={vgg.imgs: [img]})[0] for img in test_imgs]\n",
    "    test_preds = [(np.argsort(p)[::-1])[0:1] for p in prob]\n",
    "    test_preds = [p[0] for p in test_preds]\n",
    "    for i in range(len(test_preds)):\n",
    "        if(test_preds[i]==430):\n",
    "            test_preds[i]=0\n",
    "        elif(test_preds[i]==429):\n",
    "            test_preds[i]=1\n",
    "        elif(test_preds[i]==805):\n",
    "            test_preds[i]=2\n",
    "        elif(test_preds[i]==768):\n",
    "            test_preds[i]=3\n",
    "        elif(test_preds[i]==574):\n",
    "            test_preds[i]=4\n",
    "    test_img_label_onehot = tf.one_hot(indices=test_preds, depth = 5)\n",
    "    print(sess_vgg_test.run(test_img_label_onehot))\n",
    "\n",
    "    #clear\n",
    "    test_imgs = None\n",
    "    vgg = None\n",
    "    test_img_files = None\n",
    "    prob = None\n",
    "    \n",
    "    sess_vgg_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_input = tf.zeros([num_imgs,hidden,input_vec_size])\n",
    "with tf.Session() as sess_init_generator:\n",
    "    input_init = sess_init_generator.run(start_input)\n",
    "for i in range(num_imgs):\n",
    "    input_init[i][0][0] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-First_Layer-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_1') as scope:\n",
    "    with tf.variable_scope('lstm_layer_1'):\n",
    "        rnn_cell_1 = tf.contrib.rnn.BasicLSTMCell(state_size_1, reuse=None)\n",
    "        output_test_1, _ = tf.contrib.rnn.static_rnn(rnn_cell_1, tf.unstack(input_init, axis=1), dtype=tf.float32)\n",
    "        input_2 = [tf.concat([out, test_imgs_bi, test_img_label_onehot], axis=1) for out in output_test_1]\n",
    "# output_t_1 = tf.contrib.rnn.static_rnn(rnn_cell, tf.unstack(full_input, axis=1), dtype=tf.float32)\n",
    "# pred = tf.nn.softmax(tf.matmul(output1, output_w[0]) + output_b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_2') as scope:\n",
    "    with tf.variable_scope('lstm_layer_2'):\n",
    "        rnn_cell_2 = tf.contrib.rnn.BasicLSTMCell(state_size_2, reuse=None)\n",
    "        output_2, _ = tf.contrib.rnn.static_rnn(rnn_cell_2, tf.unstack(input_2, axis=0), dtype=tf.float32)\n",
    "        output_w_2 = tf.Variable(tf.truncated_normal([hidden, state_size_2, input_vec_size]))\n",
    "        output_b_2 = tf.Variable(tf.zeros([input_vec_size]))\n",
    "        pred = tf.nn.softmax(tf.matmul(output_2, output_w_2) + output_b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/model_weight_v2_2.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess_model = tf.Session()\n",
    "saver = tf.train.Saver(allow_empty=True)\n",
    "saver.restore(sess_model, filepath_ckpt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first_test = sess_model.run(pred)[0]   \n",
    "input_init[0][1] = first_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "second_test = sess_model.run(pred)[1] \n",
    "input_init[0][2] = second_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "third_test = sess_model.run(pred)[2] \n",
    "input_init[0][3] = third_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hidden):\n",
    "    result = sess_model.run(pred)\n",
    "    result_temp = result[i]\n",
    "    if i == hidden -1:\n",
    "        pass\n",
    "    else:\n",
    "        input_init[:,i+1] = result_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 6, 32)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "classes = []\n",
    "f = open('./data/description/v2/class.csv', 'r')\n",
    "csvReader = csv.reader(f)\n",
    "for row in csvReader:\n",
    "    classes.append(row)\n",
    "#     print(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 6)\n"
     ]
    }
   ],
   "source": [
    "decoded_result = np.argmax(a=result, axis=2)\n",
    "print(np.shape(decoded_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is basketball because this is round and brown color with black pattern lines <EOS> <EOS> <EOS> <EOS> \n",
      " this is basketball because this is round and brown color with black pattern lines <EOS> <EOS> <EOS> <EOS> \n",
      " this is baseball because this is round and white color with red pattern lines <EOS> <EOS> <EOS> <EOS> \n",
      " this is soccerball  because this is round and white color with black pattern lines <EOS> <EOS> <EOS> <EOS> \n",
      " this is lugbyball  because this is on and brown color with white pattern lines <EOS> <EOS> <EOS> <EOS> \n",
      " this is golfball because this is round and white color with dimply pattern <EOS> <EOS> <EOS> <EOS> <EOS> \n"
     ]
    }
   ],
   "source": [
    "for i in range(num_imgs):\n",
    "    str = \" \"\n",
    "    for r in decoded_result:\n",
    "        str += classes[r[i]][0] + \" \"\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Code Storage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def lstm_cell():\n",
    "    tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "\n",
    "    if is_training and config.keep_prob < 1:\n",
    "    output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype=data_type())\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(input_.targets, [-1])],\n",
    "        [tf.ones([batch_size * num_steps], dtype=data_type())])\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "    self._final_state = state\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sess = tf.Session()\n",
    "imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])\n",
    "vgg = vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "img1 = imread('data/img/cropped/002.png', mode='RGB')\n",
    "prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "preds = (np.argsort(prob)[::-1])[0:5]\n",
    "for p in preds:\n",
    "    print(class_names[p], prob[p])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py_tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
