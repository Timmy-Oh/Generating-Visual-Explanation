{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#File Path\n",
    "# filepath_input  = \"./data/run/\" #input csv file path\n",
    "filepath_ckpt  = \"./ckpt/model_weight.ckpt\" #weight saver check point file path\n",
    "filepath_pred = \"./output/predicted.csv\" #predicted value file path\n",
    "filename_queue_description = tf.train.string_input_producer(['./data/description/raw_data.csv'])\n",
    "num_record = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM - Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_vec_size = 5\n",
    "input_vec_size = 27\n",
    "batch_size = 50\n",
    "state_size_1 = 100\n",
    "state_size_2 = 4096 + state_size_1\n",
    "hidden = 15\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "        self.fc_layers()\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            sess.run(self.parameters[i].assign(weights[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess_vgg:\n",
    "    imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])\n",
    "    vgg = vgg16(imgs, 'vgg16_weights.npz', sess_vgg)\n",
    "    img_files = ['./data/img/cropped/' + i for i in os.listdir('./data/img/cropped')]\n",
    "    imgs = [imread(file, mode='RGB') for file in img_files]\n",
    "    temps = [sess_vgg.run(vgg.fc1, feed_dict={vgg.imgs: [imgs[i]]})[0] for i in range(50)]\n",
    "    reimgs= np.reshape(a=temps, newshape=[50,-1])\n",
    "    sess_vgg.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader()\n",
    "key,value = reader.read(filename_queue_description)\n",
    "record_defaults =[[-1], [-1], [-1], [-1], [-1], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2]]\n",
    "lab1, lab2, lab3, lab4, lab5, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15 = tf.decode_csv(value, record_defaults)  \n",
    "\n",
    "feature_label = tf.stack([lab1, lab2, lab3, lab4, lab5])\n",
    "feature_word = tf.stack([w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess_data:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    img_queue = []\n",
    "    for i in range(num_record):\n",
    "#         image = sess.run(images)\n",
    "        \n",
    "        label, raw_word = sess_data.run([feature_label, feature_word])\n",
    "        onehot = tf.one_hot(indices=raw_word, depth=27)\n",
    "        if i == 0:\n",
    "            full_input = onehot\n",
    "            full_label = label\n",
    "        else:\n",
    "            full_input = tf.concat([full_input, onehot], 0)\n",
    "            full_label = tf.concat([full_label, label], 0)\n",
    "#         print(sess.run(tf.shape(image)))\n",
    "#         batch = tf.train.batch([image, label], 1)\n",
    "#         print(sess.run(batch))\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=False)\n",
    "    images = tf.image.decode_png(value, channels=3, dtype=tf.uint8)\n",
    "    reader = tf.TextLineReader()\n",
    "    key,value = reader.read(filename_queue_description)\n",
    "    record_defaults =[[-1], [-1], [-1], [-1], [-1], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2]]\n",
    "    lab1, lab2, lab3, lab4, lab5, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15 = tf.decode_csv(value, record_defaults)  \n",
    "\n",
    "    feature_label = tf.stack([lab1, lab2, lab3, lab4, lab5])\n",
    "    feature_word = tf.stack([w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15])\n",
    "    example_batch, label_batch = tf.train.batch([images, feature_label], batch_size=batch_size)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    input_pipeline(filename_queue_description, num_epochs=1, batch_size=10)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('batch') as scope:\n",
    "    # full_label = tf.reshape(full_label, [batch_size, hidden, label_vec_size])\n",
    "    full_input = tf.reshape(full_input, [batch_size, hidden, input_vec_size])\n",
    "    input_batch, label_batch = tf.train.batch([full_input, full_input], batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_1') as scope:\n",
    "    with tf.variable_scope('lstm_layer_1'):\n",
    "        rnn_cell_1 = tf.contrib.rnn.BasicLSTMCell(state_size_1, reuse=None)\n",
    "        output_1, _ = tf.contrib.rnn.static_rnn(rnn_cell_1, tf.unstack(full_input, axis=1), dtype=tf.float32)\n",
    "#         output_w_1 = tf.Variable(tf.truncated_normal([hidden, state_size_1, input_vec_size]))\n",
    "#         output_b_1 = tf.Variable(tf.zeros([input_vec_size]))\n",
    "#         pred_temp = tf.matmul(output_1, output_w_1) + output_b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess_temp:\n",
    "    print(sess_temp.run(tf.shape(output_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_2 = [tf.concat([out, reimgs], axis=1) for out in output_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_2') as scope:\n",
    "    with tf.variable_scope('lstm_layer_2'):\n",
    "        rnn_cell_2 = tf.contrib.rnn.BasicLSTMCell(state_size_2, reuse=None)\n",
    "        output_2, _ = tf.contrib.rnn.static_rnn(rnn_cell_2, tf.unstack(input_2, axis=0), dtype=tf.float32)\n",
    "        output_w_2 = tf.Variable(tf.truncated_normal([hidden, state_size_2, input_vec_size]))\n",
    "        output_b_2 = tf.Variable(tf.zeros([input_vec_size]))\n",
    "        pred = tf.nn.softmax(tf.matmul(output_2, output_w_2) + output_b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    loss = tf.constant(0, tf.float32)\n",
    "    for i in range(hidden):\n",
    "        loss += tf.losses.softmax_cross_entropy(tf.unstack(full_input, axis=1)[i], tf.unstack(pred, axis=0)[i])\n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess_train:\n",
    "    sess_train.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess_train, filepath_ckpt)\n",
    "    \n",
    "    for i in range(31):\n",
    "        sess_train.run(train)\n",
    "        if i % 5 == 0:\n",
    "            print(\"loss : \", sess_train.run(loss))\n",
    "#             print(\"pred : \", sess.run(pred))\n",
    "    save_path = saver.save(sess_train, filepath_ckpt)\n",
    "    print(\"= Weigths are saved in \" + filepath_ckpt)\n",
    "    sess_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-be50d3c1e8f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess_vgg_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvgg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vgg16_weights.npz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess_vgg_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtest_img_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'./data/img/cropped/001.png'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_img_files\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9174cde082c2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, imgs, weights, sess)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msess\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9174cde082c2>\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, weight_file, sess)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1115\u001b[0m                 run_metadata):\n\u001b[0;32m   1116\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[1;32m-> 1166\u001b[1;33m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[0;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess_vgg_test:\n",
    "    imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])\n",
    "    vgg = vgg16(imgs, 'vgg16_weights.npz', sess_vgg_test)\n",
    "    test_img_files = ['./data/img/cropped/001.png']\n",
    "    test_imgs = [imread(file, mode='RGB') for file in test_img_files]\n",
    "#     bilinear_test_imgs = [imresize(arr=img,interp='bilinear') for img in test_imgs]\n",
    "    temps = [sess_vgg_test.run(vgg.fc1, feed_dict={vgg.imgs: [img]})[0] for img in test_imgs]\n",
    "    test_reimgs= np.reshape(a=temps, newshape=[1,-1])\n",
    "    sess_vgg_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_input = tf.zeros([1,15,27])\n",
    "with tf.Session() as sess_init_generator:\n",
    "    input_init = sess_init_generator.run(start_input)\n",
    "sos = [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "input_init[0][0] = sos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_1') as scope:\n",
    "    with tf.variable_scope('lstm_layer_1'):\n",
    "        rnn_cell_1 = tf.contrib.rnn.BasicLSTMCell(state_size_1, reuse=True)\n",
    "        output_test_1, _ = tf.contrib.rnn.static_rnn(rnn_cell_1, tf.unstack(input_init, axis=1), dtype=tf.float32)\n",
    "# output_t_1 = tf.contrib.rnn.static_rnn(rnn_cell, tf.unstack(full_input, axis=1), dtype=tf.float32)\n",
    "# pred = tf.nn.softmax(tf.matmul(output1, output_w[0]) + output_b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_2 = [tf.concat([out, test_reimgs], axis=1) for out in output_test_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm_layer_2') as scope:\n",
    "    with tf.variable_scope('lstm_layer_2'):\n",
    "        rnn_cell_2 = tf.contrib.rnn.BasicLSTMCell(state_size_2, reuse=None)\n",
    "        output_2, _ = tf.contrib.rnn.static_rnn(rnn_cell_2, tf.unstack(input_2, axis=0), dtype=tf.float32)\n",
    "        output_w_2 = tf.Variable(tf.truncated_normal([hidden, state_size_2, input_vec_size]))\n",
    "        output_b_2 = tf.Variable(tf.zeros([input_vec_size]))\n",
    "        pred = tf.nn.softmax(tf.matmul(output_2, output_w_2) + output_b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/model_weight.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess_model = tf.Session()\n",
    "saver = tf.train.Saver(allow_empty=True)\n",
    "saver.restore(sess_model, filepath_ckpt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first_test = sess_model.run(pred)[0]   \n",
    "input_init[0][1] = first_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "second_test = sess_model.run(pred)[1] \n",
    "input_init[0][2] = second_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "third_test = sess_model.run(pred)[2] \n",
    "input_init[0][3] = third_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(hidden):\n",
    "    result = sess_model.run(pred)\n",
    "    result_temp = result[i]\n",
    "    if i == hidden -1:\n",
    "        pass\n",
    "    else:\n",
    "        input_init[0][i+1] = result_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1, 27)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_result = np.argmax(a=result, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.27343693e-30\n",
      "     5.38096428e-01   1.82024389e-32   0.00000000e+00   0.00000000e+00\n",
      "     1.50284929e-23   2.51590945e-28   0.00000000e+00   5.28880257e-14\n",
      "     0.00000000e+00   0.00000000e+00   4.61901069e-01   1.28550673e-34\n",
      "     9.94173141e-14   0.00000000e+00   1.05735739e-32   5.47885447e-38\n",
      "     0.00000000e+00   2.52688324e-06   0.00000000e+00   1.55476981e-33\n",
      "     0.00000000e+00   0.00000000e+00   7.86965369e-30]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   2.86878677e-17\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.29819138e-28   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.28736996e-23   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   6.66610328e-19   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   5.50534383e-37\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  9.47464763e-14   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   5.92610893e-33   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   7.40656820e-26   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   2.70786465e-34\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.40755670e-20   0.00000000e+00   3.56167183e-36]]\n",
      "\n",
      " [[  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   1.20799324e-31   4.17731691e-36   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   7.65512594e-34   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   2.93898785e-35   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     8.12473578e-38   0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [12]\n",
      " [10]\n",
      " [16]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [21]\n",
      " [ 0]]\n"
     ]
    }
   ],
   "source": [
    "print(decoded_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Code Storage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def lstm_cell():\n",
    "    tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "\n",
    "    if is_training and config.keep_prob < 1:\n",
    "    output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype=data_type())\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(input_.targets, [-1])],\n",
    "        [tf.ones([batch_size * num_steps], dtype=data_type())])\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "    self._final_state = state\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sess = tf.Session()\n",
    "imgs = tf.placeholder(tf.float32, [None, 200, 200, 3])\n",
    "vgg = vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "img1 = imread('data/img/cropped/002.png', mode='RGB')\n",
    "prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "preds = (np.argsort(prob)[::-1])[0:5]\n",
    "for p in preds:\n",
    "    print(class_names[p], prob[p])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
